{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM20o_gewJ2j"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgATLFdeAD6i"
   },
   "source": [
    "Set directory of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xkJIgE01_9d7"
   },
   "outputs": [],
   "source": [
    "directory = '/Users/hisham/Google Drive/Recipes1M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZ6dnAJeSGGl",
    "outputId": "ff7e420f-eaf1-4c29-8a6e-e7930d670590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyIW34GyEWcI",
    "outputId": "e969cf6b-9659-494d-f219-c8cb498ec52f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hisham/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hisham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from nltk import download\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "download('wordnet')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "Vy8rliOET3O8",
    "outputId": "fc9b5c65-0df4-4b3a-d823-ab77b73468b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.0.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from gensim==4.0.1) (1.21.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from gensim==4.0.1) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from gensim==4.0.1) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "id": "TEu67aheT_9Y",
    "outputId": "5bc6b369-240a-43ec-ca74-84404398b662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93YKRg-kF0mk"
   },
   "source": [
    "# Parsing Recipe1M to ingredient list dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkruTMGXan_C"
   },
   "source": [
    "## Loading Recipe1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz8-D_Z8GH-4"
   },
   "outputs": [],
   "source": [
    "json_filename = f'{directory}/layer1.json'\n",
    "json_file = open(json_filename , 'r')\n",
    "data = json.load(json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkpdOf-LaqvJ"
   },
   "source": [
    "## Generating food names and synonym dictionaries\n",
    "Here we extract the names from the KB, normalise them, and then generate a dictionary of all known recipes. We also generate a synonym dict that maps every synonym to its main ingredient name (we'll make use of this when we parse ingredients from Recipe1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gx7FDVYsBmoT"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalise_ingredient(name):\n",
    "  if type(name) is not str:\n",
    "    return name\n",
    "  name = name.lower()\n",
    "  name = name.replace('-', ' ')\n",
    "  # remove parenthesised items\n",
    "  name = re.sub(r'\\(.*\\)', \"\", name)\n",
    "  name = [lemmatizer.lemmatize(word) for word in name.split()]\n",
    "\n",
    "  return \"_\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1JBI4yX2sw0",
    "outputId": "fc1a597b-cf08-4ecf-f919-0c453d10f833"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['green_bean', 'chorizo', 'blackcurrant', 'cornmeal', 'truffle']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = requests.get('https://ecarekb.schlegel-online.de/foodon_ids').json()\n",
    "food_names = set([normalise_ingredient(ing['ingredient']) for ing in ids])\n",
    "list(food_names)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "unwCqjuX323S"
   },
   "outputs": [],
   "source": [
    "synonyms = dict()\n",
    "for ing in ids:\n",
    "  name = normalise_ingredient(ing['ingredient'])\n",
    "  syns = [\n",
    "          normalise_ingredient(word)\n",
    "          for word in ing['alternate_names']\n",
    "          if normalise_ingredient(word) != name\n",
    "  ]\n",
    "  for word in syns:\n",
    "    synonyms[word] = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2qzrI4PBbrA"
   },
   "source": [
    "Save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVwDobEJufSC"
   },
   "outputs": [],
   "source": [
    "with open(f'{directory}/synonyms.json', 'w') as f:\n",
    "  json.dump(synonyms, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMUTIV6mu5Jh"
   },
   "outputs": [],
   "source": [
    "with open(f'{directory}/food_names.json', 'w') as f:\n",
    "  json.dump(list(food_names), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMvyQiPPBn5g"
   },
   "source": [
    "## Ingredient/instruction filtering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VKg6o110ASfG"
   },
   "outputs": [],
   "source": [
    "all_names = food_names.union(synonyms.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3wP6v-tLh2xP"
   },
   "outputs": [],
   "source": [
    "def get_name(ing):\n",
    "  if ing in food_names:\n",
    "    return ing\n",
    "  return synonyms[ing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5xzvFgKIEYqP"
   },
   "outputs": [],
   "source": [
    "def filter_ingredient(ing):\n",
    "  \"\"\"\n",
    "  Takes in a string and removes words that aren't defined ingredients\n",
    "  \"\"\"\n",
    "  ing = ing.lower()\n",
    "\n",
    "  ing = ing.replace('-', ' ')\n",
    "  ing = ing.replace(',', ' ')\n",
    "  ing = ing.replace('/', ' ')\n",
    "\n",
    "  # remove punctuation except parentheses and dashes\n",
    "  ing = ing.translate(str.maketrans('', '', string.punctuation.replace('()', \"\")))\n",
    "\n",
    "  # remove parenthesised items\n",
    "  ing = re.sub(r'\\(.*\\)', \"\", ing)\n",
    "\n",
    "  # remove fractions\n",
    "  ing = re.sub(r'\\d/\\d', \"\", ing)\n",
    "\n",
    "  # remove digits\n",
    "  ing = re.sub(r'\\d', \"\", ing)\n",
    "\n",
    "  # lemmatize words\n",
    "  words = [lemmatizer.lemmatize(word) for word in ing.split()]\n",
    "\n",
    "  # the following loop ensures multi-word ingredient names\n",
    "  # are included without including the subwords\n",
    "  ing = ''\n",
    "  i = 0\n",
    "  while i < len(words) - 2:\n",
    "    if f'{words[i]}_{words[i+1]}_{words[i+2]}' in all_names:\n",
    "      ing += get_name(f'{words[i]}_{words[i+1]}_{words[i+2]}') + ' '\n",
    "      i += 2\n",
    "    elif f'{words[i]}_{words[i+1]}' in all_names:\n",
    "      ing += get_name(f'{words[i]}_{words[i+1]}') + ' '\n",
    "      i += 1\n",
    "    elif f'{words[i+1]}_{words[i]}' in all_names:\n",
    "      ing += get_name(f'{words[i+1]}_{words[i]}') + ' '\n",
    "      i += 1\n",
    "    elif words[i] in all_names:\n",
    "      ing += get_name(words[i]) + \" \"\n",
    "    i += 1\n",
    "  # if there are 2 remaining words\n",
    "  if i == len(words) - 2:\n",
    "    if f'{words[i]}_{words[i+1]}' in all_names:\n",
    "      ing += get_name(f'{words[i]}_{words[i+1]}')\n",
    "    elif f'{words[i+1]}_{words[i]}' in all_names:\n",
    "      ing += get_name(f'{words[i+1]}_{words[i]}')\n",
    "    else:\n",
    "      if words[i] in all_names:\n",
    "        ing += get_name(words[i]) + ' '\n",
    "      if words[i+1] in all_names:\n",
    "        ing += get_name(words[i+1])\n",
    "\n",
    "  \n",
    "  # if there's 1 remaining word\n",
    "  if i == len(words) - 1:\n",
    "    if words[i] in all_names:\n",
    "      ing += get_name(words[i])\n",
    "\n",
    "  \n",
    "  return \" \".join(ing.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xWBPXhWzaszy",
    "outputId": "3434c0e9-9436-490f-a8db-ef8ce6ee05eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black_pepper'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_ingredient('black pepper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R6uGyoA7h6nr"
   },
   "outputs": [],
   "source": [
    "def filter_instruction(ins):\n",
    "  \"\"\"\n",
    "  Takes in a string and normalises the ingredients without removing other words\n",
    "  \"\"\"\n",
    "  ins = ins.lower()\n",
    "\n",
    "  ins = ins.replace('-', ' ')\n",
    "  ins = ins.replace(',', ' ')\n",
    "  ins = ins.replace('/', ' ')\n",
    "\n",
    "  # remove punctuation\n",
    "  ins = ins.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "  # remove digits\n",
    "  ins = re.sub(r'\\d', \"\", ins)\n",
    "\n",
    "  # lemmatize words and remove stopwords\n",
    "  sw = set(stopwords.words('english'))\n",
    "  words = [lemmatizer.lemmatize(word) for word in ins.split() if word not in sw]\n",
    "\n",
    "  ins = ''\n",
    "  i = 0\n",
    "  while i < len(words) - 2:\n",
    "    if f'{words[i]}_{words[i+1]}_{words[i+2]}' in all_names:\n",
    "      ins += get_name(f'{words[i]}_{words[i+1]}_{words[i+2]}') + ' '\n",
    "      i += 2\n",
    "    elif f'{words[i]}_{words[i+1]}' in all_names:\n",
    "      ins += get_name(f'{words[i]}_{words[i+1]}') + ' '\n",
    "      i += 1\n",
    "    elif f'{words[i+1]}_{words[i]}' in all_names:\n",
    "      ins += get_name(f'{words[i+1]}_{words[i]}') + ' '\n",
    "      i += 1\n",
    "    elif words[i] in all_names:\n",
    "      ins += get_name(words[i]) + \" \"\n",
    "    else:\n",
    "      ins += words[i] + \" \"\n",
    "    i += 1\n",
    "\n",
    "  # if there are 2 remaining words\n",
    "  if i == len(words) - 2:\n",
    "    if f'{words[i]}_{words[i+1]}' in all_names:\n",
    "      ins += get_name(f'{words[i]}_{words[i+1]}')\n",
    "    elif f'{words[i+1]}_{words[i]}' in all_names:\n",
    "      ins += get_name(f'{words[i+1]}_{words[i]}')\n",
    "    elif f'{words[i+1]}_{words[i]}' in all_names:\n",
    "      ins += get_name(f'{words[i+1]}_{words[i]}')\n",
    "    else:\n",
    "      ins += \" \".join(words[-2:])\n",
    "  \n",
    "  # if there's a remaining word\n",
    "  if i == len(words) - 1:\n",
    "    if words[i] in all_names:\n",
    "      ins += get_name(words[i])\n",
    "\n",
    "  return \" \".join(ins.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU9OYaCzGGnR"
   },
   "source": [
    "## Generating training corpus\n",
    "Each document having the format:\n",
    "\n",
    "\"ing_1 ing_2 ing_3\"\n",
    "\n",
    "and if with instructions:\n",
    "\n",
    "\"ing_1 ing_2 ing_3 @@ inst_1 || inst_2 || inst_3\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06XVRX3lFyT2"
   },
   "outputs": [],
   "source": [
    "def to_recipe_string_list(recipes, with_instructions=False):\n",
    "  \"\"\"\n",
    "  Generator that takes in Recipe1M format recipes and returns the normalised\n",
    "  string representation\n",
    "  \"\"\"\n",
    "  for recipe in recipes:\n",
    "    recipe_ings = []\n",
    "    for ing in recipe['ingredients']:\n",
    "      filtered_ing = filter_ingredient(ing['text'])\n",
    "      if filtered_ing:\n",
    "        recipe_ings.append(filtered_ing)\n",
    "    ing_string = \" \".join(recipe_ings)\n",
    "    if with_instructions:\n",
    "      recipe_insts = []\n",
    "      for inst in recipe['instructions']:\n",
    "        recipe_insts.append(filter_instruction(inst['text']))\n",
    "        instructions_string = \" || \".join(recipe_insts)\n",
    "      yield ing_string + \" @@ \" + instructions_string\n",
    "    else:\n",
    "      yield ing_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh3lJWMsa6Fd"
   },
   "source": [
    "Save new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCMH3ff0IUQA"
   },
   "outputs": [],
   "source": [
    "with open(f'{directory}/recipes_ingredients_and_instructions.txt', 'a') as f1:\n",
    "  with open(f'{directory}/recipes_ingredients_only.txt', 'a') as f2:\n",
    "    f1.seek(0)\n",
    "    f1.truncate()\n",
    "    f2.seek(0)\n",
    "    f2.truncate()\n",
    "    for rec in to_recipe_string_list(data, with_instructions=True):\n",
    "      ingredients = rec.split('@@')[0].strip()\n",
    "      f1.write(rec + '\\n')\n",
    "      f2.write(ingredients + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BLrOFOt5SEQ"
   },
   "source": [
    "# DIISH Model\n",
    "This section implements the [DIISH heuristic](https://www.frontiersin.org/articles/10.3389/frai.2020.621766/full) for ingredient substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsYG6665Yy5e"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MT25oYMCzCfM",
    "outputId": "0429cef0-17af-4d2b-b4e5-05a1dd5c58a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hisham/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to /Users/hisham/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.test.utils import datapath\n",
    "from nltk import download\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "download('wordnet')\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "id": "lqxy9rQM1ogr",
    "outputId": "a4e120c9-3d9b-433f-def2-475e50b0f75a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.2 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.3.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "catalogue",
         "spacy",
         "srsly",
         "thinc"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# will ask you to restart your runtime the first time\n",
    "!pip install spacy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XysaXN2lyAMr"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg770sU_qGPD"
   },
   "outputs": [],
   "source": [
    "!pip install mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BhGVOyPv-7U"
   },
   "outputs": [],
   "source": [
    "!pip install ml-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mJmSx2HY8_6"
   },
   "source": [
    "## Word2Vec Model\n",
    "Trained on ingredients concatenated with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IujmqGl2h-fJ"
   },
   "outputs": [],
   "source": [
    "sentences = LineSentence(datapath(f'{directory}/recipes_ingredients_and_instructions.txt'))\n",
    "model = Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8eEsr2ciyxJ"
   },
   "outputs": [],
   "source": [
    "model.save(f'{directory}/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JOALdbuS3Jj"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(f'{directory}/word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrZpmkB83p4Q"
   },
   "source": [
    "### W Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeYhBBpI2m8i"
   },
   "outputs": [],
   "source": [
    "def W(a, b):\n",
    "  if a not in model.wv.vocab or b not in model.wv.vocab:\n",
    "    return 0\n",
    "  return model.wv.similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW8UAY3cGzZi",
    "outputId": "639073fc-b68a-4276-d07e-3ba250edefdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69178545"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W('bay_leaf', 'thyme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXPTf8LDZFEt"
   },
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76lJm6u9os3D"
   },
   "outputs": [],
   "source": [
    "def generate_dict():\n",
    "  with open(f'{directory}/recipes_ingredients_only.txt', 'r') as f:\n",
    "    recipes = f.readlines()\n",
    "\n",
    "  recipes = [line.split() for line in recipes]\n",
    "\n",
    "  return Dictionary(documents=recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNlgtrrYpniB"
   },
   "outputs": [],
   "source": [
    "def save_dict(d):\n",
    "  d.save_as_text(f'{directory}/ingredient_dictionary_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4ZOW3OQ91mM"
   },
   "outputs": [],
   "source": [
    "dictionary = generate_dict()\n",
    "save_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OkrymnLlqSvd"
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load_from_text(f'{directory}/dictionary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9grdEJsFZL_w"
   },
   "source": [
    "## D Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lhc0hkDKVmD"
   },
   "source": [
    "### Saving co-occurrence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN9Y2VrTKb23"
   },
   "outputs": [],
   "source": [
    "def generate_cooccurrence_vectors(path=f'{directory}/co-occurrence_vectors'):\n",
    "  '''\n",
    "  Saves a file for each word containing the co-occurrence vector.\n",
    "  (used to speed up D execution)\n",
    "  '''\n",
    "  with open(f'{directory}/recipes_ingredients_only.txt', 'r') as f:\n",
    "    for word in dictionary.itervalues():\n",
    "      vector = np.zeros(len(dictionary))\n",
    "      recipe_count = 0\n",
    "\n",
    "      for i, line in enumerate(f):\n",
    "        ings = line.split()\n",
    "        if word in ings:\n",
    "          recipe_count += 1\n",
    "          for ing in ings:\n",
    "            vector[dictionary.token2id[ing]] += 1\n",
    "\n",
    "      if recipe_count != 0:\n",
    "        vector = vector/recipe_count\n",
    "\n",
    "      np.savetxt(f'{path}/{word}.npy', vector)\n",
    "      f.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBBOn_jkQV8y"
   },
   "outputs": [],
   "source": [
    "def get_cooccurrence_matrix(path=f'{directory}/co-occurrence_vectors'):\n",
    "  '''\n",
    "  Loads all co-occurrence vectors into memory for easy access\n",
    "  '''\n",
    "  matrix = []\n",
    "  for i in range(len(dictionary)):\n",
    "    vector = np.loadtxt(f'{path}/{dictionary[i]}.npy')\n",
    "    matrix.append(vector)\n",
    "  return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s458SQ8LQbNi"
   },
   "outputs": [],
   "source": [
    "generate_cooccurrence_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ly9PFf-yRyQ-"
   },
   "outputs": [],
   "source": [
    "np.savetxt(f'{directory}/cooccurrence_matrix.npy', get_cooccurrence_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPPOosjTSB5_"
   },
   "outputs": [],
   "source": [
    "co_occ = np.loadtxt(f'{directory}/cooccurrence_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riNWmwzLka-Q"
   },
   "outputs": [],
   "source": [
    "def D(a, b, matrix=co_occ):\n",
    "  '''\n",
    "  Can be used to calculate D score if matrix hasn't been loaded yet by setting\n",
    "  the matrix parameter to None.\n",
    "  '''\n",
    "  if not matrix is None:\n",
    "    a_vector = co_occ[dictionary.token2id[a]]\n",
    "    b_vector = co_occ[dictionary.token2id[b]]\n",
    "  \n",
    "  else:\n",
    "    a_vector, b_vector = np.zeros(len(dictionary)), np.zeros(len(dictionary))\n",
    "    a_recipe_count, b_recipe_count = 0, 0\n",
    "    with open(f'{directory}/recipes_ingredients_only.txt', 'r') as f:\n",
    "      for i, line in enumerate(f):\n",
    "        ings = line.split()\n",
    "        if a in ings:\n",
    "          a_recipe_count += 1\n",
    "          for ing in ings:\n",
    "            a_vector[dictionary.token2id[ing]] += 1\n",
    "\n",
    "        if b in ings:\n",
    "          b_recipe_count += 1\n",
    "          for ing in ings:\n",
    "            b_vector[dictionary.token2id[ing]] += 1\n",
    "\n",
    "    if a_recipe_count != 0:\n",
    "      a_vector = a_vector/a_recipe_count\n",
    "\n",
    "    if b_recipe_count != 0:\n",
    "      b_vector = b_vector/b_recipe_count\n",
    "\n",
    "\n",
    "  if np.count_nonzero(a_vector) == 0 or np.count_nonzero(b_vector) == 0:\n",
    "    return 0\n",
    "\n",
    "  \n",
    "\n",
    "  return 1 - distance.cosine(a_vector, b_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdoM-E2-EcTz",
    "outputId": "4c054857-5795-4f78-9827-fcfaaa0bca7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6602318567679469"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D('tuna', 'salmon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFPItBZkZrDl"
   },
   "source": [
    "## P Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoOKWVZKaHqb"
   },
   "source": [
    "### Generating context count vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ5XC8YvaUkX"
   },
   "source": [
    "#### Fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnPEzPyjKuDu"
   },
   "outputs": [],
   "source": [
    "def get_fc():\n",
    "  fc = np.zeros((len(dictionary), len(dictionary)))\n",
    "  with open(f'{directory}/recipes_ingredients_only.txt', 'r') as f:\n",
    "    for line in f:\n",
    "      ings = line.split()\n",
    "      for a, b in combinations(ings, 2):\n",
    "        fc[dictionary.token2id[a]][dictionary.token2id[b]] += 1\n",
    "  return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5X6x8ayEwnN"
   },
   "outputs": [],
   "source": [
    "fc = get_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18jJ1nQxbfTO"
   },
   "outputs": [],
   "source": [
    "np.savetxt(f'{directory}/context_counts.npy', fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG3mNEYQae2N"
   },
   "source": [
    "#### Fic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaS9_R13eH51"
   },
   "outputs": [],
   "source": [
    "def generate_context_vectors(path=f'{directory}/ingredient_context_counts'):\n",
    "  if not os.path.exists(path):\n",
    "\t\tos.mkdir(path)\n",
    "  with open(f'{directory}/recipes_ingredients_only.txt', 'r') as f:\n",
    "    for word in dictionary.itervalues():\n",
    "      fic = np.zeros((len(dictionary), len(dictionary)))\n",
    "      for num, line in enumerate(f):\n",
    "        ings = line.split()\n",
    "        if word in ings:\n",
    "          for context in combinations(ings, 2):\n",
    "            index = (dictionary.token2id[context[0]],dictionary.token2id[context[1]])\n",
    "            fic[index] += 1\n",
    "      np.savetxt(f'{path}/{word}.npy', fic)\n",
    "      f.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW2kPRD7YnbN"
   },
   "outputs": [],
   "source": [
    "def get_fic_matrix(path=f'{directory}/ingredient_context_counts'):\n",
    "  matrix = []\n",
    "  for i in range(len(dictionary)):\n",
    "    m = np.loadtxt(f'{path}/{dictionary[i]}.npy')\n",
    "    matrix.append(m)\n",
    "  return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qd0R45f7U4_i"
   },
   "outputs": [],
   "source": [
    "generate_context_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mt7FwWQjYwVR"
   },
   "outputs": [],
   "source": [
    "fic = get_fic_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rFTOwcvZscb"
   },
   "outputs": [],
   "source": [
    "np.save(f'{directory}/fic_matrix.npy', fic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj3nnltiamMP"
   },
   "source": [
    "### PPMI and P function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3RtVuU3ayX0"
   },
   "outputs": [],
   "source": [
    "def PPMI(fic, fi, fc):\n",
    "  b = fi * fc\n",
    "  return np.maximum(np.log10(np.divide(fic * len(dictionary) * len(fc), b, out=np.zeros(fic.shape, dtype=float), where=b!=0))\n",
    "  * np.sqrt(np.maximum(fi, fc)), np.zeros(len(fc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dgf8t0Hg1uF"
   },
   "outputs": [],
   "source": [
    "fc = np.loadtxt(f'{directory}/context_counts.npy')\n",
    "fic = np.load(f'{directory}/fic_matrix.npy')\n",
    "def P(a, b):\n",
    "\n",
    "  fic_a = fic[dictionary.token2id[a]]\n",
    "  fic_b = fic[dictionary.token2id[b]]\n",
    "\n",
    "  fi_a = dictionary.dfs[dictionary.token2id[a]]\n",
    "  fi_b = dictionary.dfs[dictionary.token2id[b]]\n",
    "\n",
    "  ppmi = PPMI(fic_a.flatten(), fi_a, fc.flatten()), PPMI(fic_b.flatten(), fi_b, fc.flatten())\n",
    "\n",
    "  if np.count_nonzero(ppmi[0]) == 0 or np.count_nonzero(ppmi[1]) == 0:\n",
    "    return 0\n",
    "  \n",
    "  return 1 - distance.cosine(ppmi[0], ppmi[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "memoFCSudE9H",
    "outputId": "2532d414-0448-425c-d899-6293295db055"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log10\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5856681025085629"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P('tuna', 'salmon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOELNb1_U_gf",
    "outputId": "14299911-b79e-434e-b68a-a60b9a657b2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log10\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26148481301231197"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P('aubergine', 'egg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myp431N5btKk"
   },
   "source": [
    "## Spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz41P8ibpp1P"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOcjjczK3izL"
   },
   "source": [
    "### S score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYIypLi4yWwZ"
   },
   "outputs": [],
   "source": [
    "nlps = {}\n",
    "for ing in dictionary.token2id:\n",
    "  ing = ing.replace('_', ' ')\n",
    "  nlps[ing] = nlp(ing)\n",
    "\n",
    "def S(a, b):\n",
    "  # for multi-word ingredients\n",
    "  a = a.replace('_', ' ')\n",
    "  b = b.replace('_', ' ')\n",
    "  return nlps[a].similarity(nlps[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJokbqhfEQVx",
    "outputId": "4f8a13a9-3409-45d3-ec16-864771338957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6643049684318862"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S('vegetable_oil', 'butter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d3L2Wj830gG"
   },
   "source": [
    "## DIISH Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hefUQXIx34bA"
   },
   "outputs": [],
   "source": [
    "def DIISH(a, b):\n",
    "  return W(a, b) + (S(a, b) ** 2) + (0.5 * D(a, b) ** 0.25) + (2 * P(a, b) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moYN4W9DcGwc",
    "outputId": "639c1df5-d830-4142-f1a3-22a9cba294ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log10\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5720430224932502"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIISH('lard', 'butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzM9nJauA8Lh",
    "outputId": "e5ac70ee-0045-46bb-93c1-3524e3797d85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log10\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.719364898570073"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIISH('tuna', 'coconut')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEklfjvcgZre"
   },
   "source": [
    "## Generating DIISH Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBchLVl3Pszc"
   },
   "outputs": [],
   "source": [
    "def generate_DIISH_matrix(path=f'{directory}/DIISH_scores'):\n",
    "  DIISH_matrix = np.zeros((len(dictionary), len(dictionary)))\n",
    "  for i in range(len(dictionary)):\n",
    "    for j in range(len(dictionary)):\n",
    "      DIISH_matrix[i][j] = DIISH(dictionary[i], dictionary[j])\n",
    "    np.savetxt(f'{path}/{dictionary[i]}.npy', DIISH_matrix[i])\n",
    "    print(f'Word {i} done')\n",
    "  return DIISH_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66U95jPHK8Fv"
   },
   "outputs": [],
   "source": [
    "DIISH_matrix = generate_DIISH_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9Yq-4ndP0wg"
   },
   "outputs": [],
   "source": [
    "np.savetxt(f'{directory}/DIISH_matrix.npy', DIISH_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmYb5_o6RSTw"
   },
   "source": [
    "## Getting Top Substitution Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ykKFQmhMSuQH"
   },
   "outputs": [],
   "source": [
    "DIISH_matrix = np.loadtxt(f'{directory}/DIISH_matrix.npy')\n",
    "def get_top_candidates(a, k=10):\n",
    "  scores = DIISH_matrix[dictionary.token2id[a]]\n",
    "  scores = [(dictionary[i], score) for i, score in enumerate(scores) if score == score]\n",
    "  return sorted(scores, key=lambda x: x[1], reverse=True)[1:k+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KK8qj0S35ZT3",
    "outputId": "942fe992-b801-42e4-f9da-916564402aa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('evaporated_milk', 3.3692197987602963),\n",
       " ('butter', 3.217424772732188),\n",
       " ('egg', 3.179688533087118),\n",
       " ('flour', 3.150331974983061),\n",
       " ('cream', 3.1492243696733864),\n",
       " ('sugar', 3.1416614889123604),\n",
       " ('soured_cream', 3.088015758484327),\n",
       " ('double_cream', 3.067865237519814),\n",
       " ('buttermilk', 3.0621373331801607),\n",
       " ('margarine', 2.972302804994131)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_candidates('milk', k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPxErqM78ork"
   },
   "source": [
    "## Evaluation\n",
    "Using data from [The Cook's Thesaurus](http://foodsubs.com/) scraped using https://github.com/solashirai/FoodSubstitutionDataScripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J2qZGXnJ54Gt"
   },
   "outputs": [],
   "source": [
    "with open(f'{directory}/scraped_thesaurus_substitutions.json', 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5zhYls1LL8D",
    "outputId": "2ac2e52f-ce36-49f4-a79c-52fa5f44ad1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'amaranth\\xa0= amaranth seeds',\n",
       "  'millet OR quinoa OR buckwheat groats',\n",
       "  'scraped_pages/subs_www.foodsubs.com_Grainoth.html.html'],\n",
       " ['1',\n",
       "  'black quinoa',\n",
       "  'quinoa',\n",
       "  'scraped_pages/subs_www.foodsubs.com_Grainoth.html.html'],\n",
       " ['2',\n",
       "  'millet',\n",
       "  'quinoa OR bulgur OR       couscous',\n",
       "  'scraped_pages/subs_www.foodsubs.com_Grainoth.html.html'],\n",
       " ['3',\n",
       "  'psyllium seed husks = PSH = plantago seed husks       = flea seed',\n",
       "  'oat       bran',\n",
       "  'scraped_pages/subs_www.foodsubs.com_Grainoth.html.html'],\n",
       " ['4',\n",
       "  'quinoa = hie',\n",
       "  'couscous OR rice OR bulgur OR millet OR buckwheat groats OR amaranth',\n",
       "  'scraped_pages/subs_www.foodsubs.com_Grainoth.html.html']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t_YzmEzA9ma9"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def filter_scraped_ingredient(ing):\n",
    "  ing = filter_ingredient(ing)\n",
    "\n",
    "  # it has to be just one ingredient (or else it means it's a multi-word\n",
    "  # ingredient that isn't in our dictionary)\n",
    "  if len(ing.split()) != 1:\n",
    "    return ''\n",
    "\n",
    "  return ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uD2Z0gPKD08X"
   },
   "outputs": [],
   "source": [
    "substitutions = defaultdict(set)\n",
    "\n",
    "for sub in data:\n",
    "  sub_from = [filter_scraped_ingredient(word) for word in sub[1].strip().split('=')]\n",
    "  sub_from = set([ing for ing in sub_from if ing != ''])\n",
    "\n",
    "  # if ingredients are undefined in our dictionary, go on to next substitution\n",
    "  if not sub_from:\n",
    "    continue\n",
    "\n",
    "  sub_to = [filter_scraped_ingredient(word) for word in sub[2].strip().split('OR')]\n",
    "  sub_to = set([ing for ing in sub_to if ing != ''])\n",
    "\n",
    "  if not sub_to:\n",
    "    continue\n",
    "  \n",
    "  for ing in sub_from:\n",
    "    substitutions[ing] = substitutions[ing].union(sub_to) - set([ing])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59NZmJp3D9sf",
    "outputId": "15fd9ffe-6f5f-44a3-f708-b919cc0bd665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amaranth: {'spinach', 'quinoa', 'buckwheat'}\n",
      "quinoa: {'buckwheat', 'couscous', 'rice', 'oat', 'bulgur_wheat', 'amaranth'}\n",
      "cheese: {'mozzarella', 'parmesan', 'yeast', 'cheddar', 'stilton', 'camembert', 'tofu', 'gouda', 'cottage_cheese', 'port_salut', 'monterey_jack', 'jackfruit', 'port', 'brie', 'ricotta', 'feta'}\n",
      "gouda: {'jackfruit', 'edam'}\n",
      "halloumi: {'mozzarella', 'feta'}\n",
      "mozzarella: {'cheddar', 'tofu', 'cheese'}\n",
      "port: {'beef', 'wine', 'jackfruit', 'vermouth'}\n",
      "port_salut: {'jackfruit'}\n",
      "scallop: {'skate', 'shrimp', 'crab', 'sole', 'flounder', 'squash', 'acorn_squash', 'lobster', 'monkfish', 'shark', 'cod'}\n",
      "crayfish: {'lobster', 'shrimp', 'langoustine', 'crab'}\n"
     ]
    }
   ],
   "source": [
    "for ing in list(substitutions.keys())[:10]:\n",
    "  print(f'{ing}: {substitutions[ing]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODBpq5HWphAD",
    "outputId": "639593ac-f0d7-45a8-92ec-c582e7ebb98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ricotta', 2.2897158383543754),\n",
       " ('guava', 2.15571825242428),\n",
       " ('asparagus', 2.1311106736682177),\n",
       " ('cheddar', 1.9967341464754786),\n",
       " ('dressing', nan),\n",
       " ('pimento', nan),\n",
       " ('salad', nan),\n",
       " ('melon', 1.9739713028076609),\n",
       " ('corn_oil', 1.9608168077126802),\n",
       " ('pomegranate', 1.9390146108687403)]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_candidates('halloumi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ktYSZNiIvRK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  threshold: a success is whenever the model predicts at least\n",
    "    threshold% of the possible substitutions\n",
    "\n",
    "  k: testing top k candidates from the model\n",
    "'''\n",
    "def get_map(threshold=0.5, k=5):\n",
    "  success_count = 0\n",
    "  success_threshold_length = ceil(threshold * k)\n",
    "  failures = dict()\n",
    "  for ing, subs in substitutions.items():\n",
    "    candidates = set([c[0] for c in get_top_candidates(ing, k=k)])\n",
    "    matches = subs.intersection(candidates)\n",
    "    if len(matches) >= success_threshold_length \\\n",
    "        or len(matches) == len(subs):\n",
    "      success_count += 1\n",
    "    else:\n",
    "      failures[ing] = subs\n",
    "  accuracy = success_count / len(substitutions)\n",
    "  return accuracy, failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Ig6_ZWWZzXa",
    "outputId": "e350e5ae-865c-4aa4-9545-0f3b805dace8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23115577889447236"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_map()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTAMZi_gLaIz"
   },
   "outputs": [],
   "source": [
    "# looking at failures\n",
    "get_map()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V3UEpd614o-"
   },
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyePCG4m57Kk"
   },
   "source": [
    "## TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "On2NtQK56AAk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.models import TfidfModel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TFIDFVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def __init__(self, dict_path=None, model_path=None):\n",
    "    self.dict_path = dict_path\n",
    "    self.model_path = model_path\n",
    "    self.id2word = None\n",
    "    self.tfidf = None\n",
    "    self.load()\n",
    "\n",
    "  def load(self):\n",
    "    if self.dict_path != None and os.path.exists(self.dict_path):\n",
    "      self.id2word = Dictionary.load_from_text(self.dict_path)\n",
    "    if self.model_path != None and os.path.exists(self.model_path):\n",
    "      self.tfidf = TfidfModel.load(self.model_path)\n",
    "\n",
    "  def save(self):\n",
    "    if self.dict_path != None:\n",
    "      self.id2word.save_as_text(self.dict_path)\n",
    "    if self.model_path != None:\n",
    "      self.tfidf.save(self.model_path)\n",
    "\n",
    "  def fit(self, documents, labels=None):\n",
    "    self.id2word = Dictionary(documents)\n",
    "    # filter ingredients that occur less than 5 times or in more than 70% of the\n",
    "    # recipes, then keep only the 1500 most frequent ingredients\n",
    "    # self.id2word.filter_extremes(no_below=5, no_above=0.8, keep_n=400)\n",
    "    self.tfidf = TfidfModel(dictionary=self.id2word, normalize=True)\n",
    "    self.save()\n",
    "    return self\n",
    "\n",
    "  def transform(self, documents):\n",
    "    for document in documents:\n",
    "      docvec = self.tfidf[self.id2word.doc2bow(document)]\n",
    "      yield sparse2full(docvec, len(self.id2word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnBU5nfKW7fT"
   },
   "source": [
    "## Doc2Vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GHIH-La4XG9Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class Doc2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.model = None\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        if self.path != None:\n",
    "            self.model = Doc2Vec.load(self.path)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def fit(self,\n",
    "            documents=None,\n",
    "            corpus_file=None,\n",
    "            vector_size=600,\n",
    "            min_count=5,\n",
    "            seed=1,\n",
    "            workers=2):\n",
    "        if corpus_file is None:\n",
    "            corpus = [\n",
    "                TaggedDocument(words, [idx])\n",
    "                for idx, words in enumerate(documents)\n",
    "            ]\n",
    "            self.model = Doc2Vec(corpus,\n",
    "                                 vector_size=vector_size,\n",
    "                                 min_count=min_count,\n",
    "                                 seed=seed,\n",
    "                                 workers=workers)\n",
    "        else:\n",
    "            self.model = Doc2Vec(corpus_file=corpus_file,\n",
    "                                 vector_size=vector_size,\n",
    "                                 min_count=min_count,\n",
    "                                 seed=seed,\n",
    "                                 workers=workers)\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.model.infer_vector(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKGRM8CGpxnM"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHmdvqLmgHen"
   },
   "source": [
    "### Training TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgF3OCbEenOo"
   },
   "source": [
    "Train TF-IDF transformer on the filtered dataset (ingredients only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tlhiQSy1p21p"
   },
   "outputs": [],
   "source": [
    "filename = f'{directory}/recipes_ingredients_only.txt'\n",
    "with open(filename, 'r') as f:\n",
    "  data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7at_3ZufAPj"
   },
   "source": [
    "Tokenize every recipe before passing it to TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xWMY8kTFv_q_"
   },
   "outputs": [],
   "source": [
    "def tokenize(recipes):\n",
    "  for recipe in recipes:\n",
    "    yield recipe.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "crU0K4wT7Bo7"
   },
   "outputs": [],
   "source": [
    "tokenized = list(tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s5BuLFMCfvlC"
   },
   "outputs": [],
   "source": [
    "tfidf = TFIDFVectorizer(\n",
    "    model_path=f'{directory}/tfidf_model_ingredients_only',\n",
    "    dict_path=f'{directory}/dictionary.txt'\n",
    "    )\n",
    "\n",
    "# tfidf.fit(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMl-6KhWMdnr"
   },
   "outputs": [],
   "source": [
    "# tfidf.tfidf.save(f'{directory}/tfidf_model_ingredients_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6_Y_GS8S3PO"
   },
   "source": [
    "Generate the vectors for each recipe in Recipe1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8ocRQRyaqWDg"
   },
   "outputs": [],
   "source": [
    "tfidf_vecs = list(tfidf.transform(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij7DfzScS9wr"
   },
   "source": [
    "Save the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-rzwa1Eeibn"
   },
   "outputs": [],
   "source": [
    "np.savetxt(f'{directory}/tfidf_vectors_ingredients_only.gz', tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "PnRqbaRghskk"
   },
   "outputs": [],
   "source": [
    "tfidf_vecs = np.loadtxt(f'{directory}/tfidf_vectors_ingredients_only.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgiXegjPaQW0"
   },
   "source": [
    "### Training Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RLNI7KTlPrmf"
   },
   "outputs": [],
   "source": [
    "doc2vec = Doc2VecVectorizer().fit(corpus_file=f'{directory}/recipes_ingredients_and_instructions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WZa9w7PzPSo-"
   },
   "outputs": [],
   "source": [
    "doc2vec = Doc2VecVectorizer(f'{directory}/doc2vec_model_ingredients_and_instructions.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "L5xF5tG8QYCK"
   },
   "outputs": [],
   "source": [
    "doc2vec.save(f'{directory}/doc2vec_model_ingredients_and_instructions.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsEpL_dWAtNM"
   },
   "source": [
    "Generate recipe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "CXFV__2naknS"
   },
   "outputs": [],
   "source": [
    "doc2vec_vecs = []\n",
    "with open(f'{directory}/recipes_ingredients_and_instructions.txt', 'r') as f:\n",
    "  for _, line in enumerate(f):\n",
    "    line = line.split()\n",
    "    doc2vec_vecs.append(doc2vec.model.infer_vector(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0yccg1Xx2Gz"
   },
   "outputs": [],
   "source": [
    "np.savetxt(f'{directory}/doc2vec_vectors_ingredients_and_instructions.gz', doc2vec_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "l0tsrO04UpYN"
   },
   "outputs": [],
   "source": [
    "doc2vec_vecs = np.loadtxt(f'{directory}/doc2vec_vectors_ingredients_and_instructions.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FF46oJsd0l3"
   },
   "outputs": [],
   "source": [
    "len(doc2vec_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_RwyvoT2ThW"
   },
   "source": [
    "# Recipe Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZnXES9EIgEi9"
   },
   "outputs": [],
   "source": [
    "def split_array_ranges(length, k):\n",
    "  \"\"\"\n",
    "  Takes in a length of a list and returns a list of index tuples covering k chunks\n",
    "  \"\"\"\n",
    "  chunks = []\n",
    "  step = int(length/k)\n",
    "  start_ind = 0\n",
    "  end_ind = step\n",
    "  while end_ind < length:\n",
    "    chunks.append((start_ind, end_ind))\n",
    "    start_ind = end_ind\n",
    "    end_ind += step\n",
    "  chunks.append((start_ind, length))\n",
    "  return chunks\n",
    "\n",
    "def get_most_similar(docvec, docvecs, k = 20, n_clusters = 10):\n",
    "  \"\"\"\n",
    "  Gets the k most similar recipes\n",
    "\n",
    "  Parameters:\n",
    "    docvec: the TF-IDF vector of the queried recipe\n",
    "    docvecs: the TF-IDF vectors of the corpus\n",
    "    k\n",
    "    n_clusters: how many times to split the data\n",
    "\n",
    "  Returns\n",
    "    a k-length list of (index, distance) tuples sorted by distance\n",
    "  \"\"\"\n",
    "  # cut data to n_clusters number of clusters\n",
    "  similar_recipes = []\n",
    "  for start, end in split_array_ranges(len(docvecs), n_clusters):\n",
    "    if end - start < k:\n",
    "      break\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='cosine', algorithm='auto').fit(docvecs[start:end])\n",
    "    distances, indicies = nbrs.kneighbors([docvec])\n",
    "    indicies = list(map(lambda x: x+start, indicies))\n",
    "    for x in zip(indicies[0], distances[0]):\n",
    "      similar_recipes.append(x)\n",
    "  return sorted(similar_recipes, key=lambda x: x[1])[:k]\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# SIZE = 1029720\n",
    "\n",
    "# def get_most_similar_gen(docvec, docvecs, k = 20, n_clusters = 10):\n",
    "#   # cut data to n_clusters number of clusters\n",
    "#   similar_recipes = []\n",
    "#   for start, end in split_array_ranges(SIZE, n_clusters):\n",
    "#     if end - start < k:\n",
    "#       break\n",
    "#     vectors = list(itertools.islice(docvecs, end - start))\n",
    "#     nbrs = NearestNeighbors(n_neighbors=k, metric='cosine', algorithm='auto').fit(vectors)\n",
    "#     distances, indicies = nbrs.kneighbors([docvec])\n",
    "#     indicies = list(map(lambda x: x+start, indicies))\n",
    "#     for x in zip(indicies[0], distances[0]):\n",
    "#       similar_recipes.append(x)\n",
    "#   return sorted(similar_recipes, key=lambda x: x[1])[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33NXqluodFlp"
   },
   "source": [
    "# Ingredient Substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhGUEkA9VEJE"
   },
   "source": [
    "## GHG dictionary loading\n",
    "Load in the ingredient carbon data from the API into a dictionary, taking into account alternate ingredient names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ogXGLFRGSocu"
   },
   "outputs": [],
   "source": [
    "ghg_dict = defaultdict(float)\n",
    "ids = requests.get('https://ecarekb.schlegel-online.de/foodon_ids').json()\n",
    "for ing in ids:\n",
    "  name = filter_ingredient(ing['ingredient'])\n",
    "  if name:\n",
    "    req = requests.get(f'https://ecarekb.schlegel-online.de/ingredient?ingredient={\"+\".join(ing[\"ingredient\"].split())}')\n",
    "    ghg = req.json()['ghg']\n",
    "    ghg_dict[name] = ghg\n",
    "  else:\n",
    "    continue\n",
    "  for alt_name in ing['alternate_names']:\n",
    "    alt_name = filter_ingredient(alt_name)\n",
    "    if alt_name and alt_name not in ghg_dict:\n",
    "      ghg_dict[alt_name] = ghg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiRvTozEZK_J"
   },
   "source": [
    "## Final substitution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pO1iZIWHbP6w"
   },
   "outputs": [],
   "source": [
    "def get_substitutions(ingredients: [str],\n",
    "                      model,\n",
    "                      vecs,\n",
    "                      instructions: [str]=[],\n",
    "                      verbose=False):\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "    ingredients: a list of ingredient strings\n",
    "    model: vectorizer to use to get the recipe vector\n",
    "    instructions: a list of instruction strings (optional)\n",
    "\n",
    "  Returns:\n",
    "    a list of tuples in the format of\n",
    "    (original_ingredient, substitution, confidence)\n",
    "    sorted by confidence\n",
    "  \"\"\"\n",
    "\n",
    "  # filter ingredients and instructions and then tokenize them\n",
    "  ingredients = [filter_ingredient(ing) for ing in ingredients]\n",
    "  instructions = filter_instruction(\" || \".join(instructions)).split()\n",
    "\n",
    "  # concatenate the two using @@ if there are instructions\n",
    "  if instructions:\n",
    "    recipe = ingredients + ['@@'] + instructions\n",
    "  else:\n",
    "    recipe = ingredients\n",
    "\n",
    "  # get recipe vector\n",
    "  recipe_vec = next(model.transform([recipe]))\n",
    "\n",
    "  # get the most similar recipes\n",
    "  similar_recipes = get_most_similar(recipe_vec, vecs)\n",
    "  if verbose:\n",
    "    print('Similar recipes (index, confidence): ', similar_recipes)\n",
    "\n",
    "  # uncomment the next line out if data is NOT loaded\n",
    "  # load the recipes' ingredients and tokenize them\n",
    "  # recipes_ind = [x[0] for x in similar_recipes]\n",
    "  # recipes = []\n",
    "  # with open(f'{directory}/recipes_ingredients_only.txt') as f:\n",
    "  #   for i, line in enumerate(f):\n",
    "  #     if i in recipes_ind:\n",
    "  #       recipes.append(line.split())\n",
    "\n",
    "  # uncomment the next line out if data is loaded\n",
    "  recipes = [data[index].split() for index, _ in similar_recipes]\n",
    "\n",
    "  if verbose:\n",
    "    print('Recipes\\' ingredients:', recipes)\n",
    "\n",
    "  # get the important and substitutable ingredients\n",
    "  imp, subs = get_substitutable_ings(recipes)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"Important: \", imp)  \n",
    "    print(\"Substitutable: \", subs)  \n",
    "\n",
    "  substitutions = []\n",
    "  # loop through every ingredients in the passed recipe\n",
    "  for ingredient in ingredients:\n",
    "    # if it's substitutable in the recipe,\n",
    "    if ingredient in subs:\n",
    "      # check if the ingredient substitution model outputs something that\n",
    "      # is also substitutable in the recipe\n",
    "      similar_ingredients = get_top_candidates(ingredient, k=5)\n",
    "      for sim_ing, confidence in similar_ingredients:\n",
    "        # add it to the list of possible substitutions if it is\n",
    "        if sim_ing in subs and sim_ing not in ingredients:\n",
    "          substitutions.append({'from': ingredient, 'to': sim_ing, 'confidence': confidence})\n",
    "\n",
    "  # remove duplicates\n",
    "  substitutions = [dict(t) for t in {tuple(s.items()) for s in substitutions}]\n",
    "  # sort by how confident we are of the substitution being a viable one\n",
    "  substitutions.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "  # calculate total recipe carbon\n",
    "  total_ghg = sum([ghg_dict[ing] for ing in ingredients])\n",
    "\n",
    "  # only return substitutions of ingredients that are high\n",
    "  # carbon (>= 20% or total recipe ghg) and if the subtitute has a less ghg\n",
    "  substitutions = list(filter(\n",
    "      lambda sub: ghg_dict[sub['from']] >= ghg_dict[sub['to']] and\n",
    "      ghg_dict[sub['from']] >= 0.2 * total_ghg,\n",
    "      substitutions\n",
    "      )\n",
    "  )\n",
    "\n",
    "  # add ghg difference and percent reduction to substitutions\n",
    "  for sub in substitutions:\n",
    "    sub['ghg_difference'] = ghg_dict[sub['from']] - ghg_dict[sub['to']]\n",
    "    if total_ghg == 0:\n",
    "      sub['percent_reduction'] = 0\n",
    "    else:\n",
    "      sub['percent_reduction'] = sub['ghg_difference'] / total_ghg * 100\n",
    "\n",
    "  return substitutions\n",
    "\n",
    "def get_substitutable_ings(recipes, no_above = 0.8):\n",
    "  \"\"\"\n",
    "  Seperates the important ingredients from the substituable one\n",
    "\n",
    "  Parameters:\n",
    "    recipes: list of *tokenized* recipes\n",
    "    no_above: the minimum fraction to be considered important\n",
    "\n",
    "  Returns\n",
    "    important_ings\n",
    "    subs_ings\n",
    "  \"\"\"\n",
    "  id2word = Dictionary(recipes)\n",
    "  all_ings = list(id2word.values())\n",
    "  id2word.filter_extremes(no_below=0, no_above=no_above)\n",
    "  # after filtering (substitutable)\n",
    "  subs_ings = list(id2word.values())\n",
    "  important_ings = list(filter(lambda x: x not in subs_ings, all_ings))\n",
    "  return important_ings, subs_ings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z4sH5YhZaG9"
   },
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YAgl23tIMC9K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'parmesan',\n",
       "  'to': 'parsley',\n",
       "  'confidence': 3.420124389639847,\n",
       "  'ghg_difference': 20.802999999999997,\n",
       "  'percent_reduction': 61.18169519440033},\n",
       " {'from': 'parmesan',\n",
       "  'to': 'basil',\n",
       "  'confidence': 3.372572536294494,\n",
       "  'ghg_difference': 20.802999999999997,\n",
       "  'percent_reduction': 61.18169519440033},\n",
       " {'from': 'parmesan',\n",
       "  'to': 'cheddar',\n",
       "  'confidence': 3.357463172007058,\n",
       "  'ghg_difference': 0.0,\n",
       "  'percent_reduction': 0.0},\n",
       " {'from': 'parmesan',\n",
       "  'to': 'cheese',\n",
       "  'confidence': 3.272324465175727,\n",
       "  'ghg_difference': 0.0,\n",
       "  'percent_reduction': 0.0}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = [\n",
    "  \"1 Tbsp Vegetable Oil\",\n",
    "  \"3 Cloves Garlic\",\n",
    "  \"1 Onion\",\n",
    "  \"1 Carrot\",\n",
    "  \"1 Stick Celery\",\n",
    "  \"1 Tsp Marmite\",\n",
    "  \"1 Vegetable Stock Cube\",\n",
    "  \"1 Can Tomatoes,Chopped\",\n",
    "  \"100ml Red Wine\",\n",
    "  \"125g Dehydrated Soya Mince\",\n",
    "  \"400g Spaghetti\",\n",
    "  \"1 Tbsp Parmesan Cheese\"\n",
    "]\n",
    "\n",
    "get_substitutions(ingredients, tfidf, tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "qXMlYczfBj1L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'parmesan',\n",
       "  'to': 'parsley',\n",
       "  'confidence': 3.420124389639847,\n",
       "  'ghg_difference': 20.802999999999997,\n",
       "  'percent_reduction': 51.47473647745829},\n",
       " {'from': 'parmesan',\n",
       "  'to': 'basil',\n",
       "  'confidence': 3.372572536294494,\n",
       "  'ghg_difference': 20.802999999999997,\n",
       "  'percent_reduction': 51.47473647745829},\n",
       " {'from': 'parmesan',\n",
       "  'to': 'cheese',\n",
       "  'confidence': 3.272324465175727,\n",
       "  'ghg_difference': 0.0,\n",
       "  'percent_reduction': 0.0}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = [\n",
    "  \"Spaghetti\",\n",
    "  \"Bacon\",     \n",
    "  \"Olive oil\",\n",
    "  \"Egg\",\n",
    "  \"Parmesan\"\n",
    "]\n",
    "\n",
    "instructions = [\n",
    "  \"Cook the pasta in a pan of boiling salted water according to the packet instructions.\",\n",
    "  \"Slice the bacon and place in a non-stick frying pan on a medium heat with half a tablespoon of olive oil and a really good pinch of black pepper. Leave it to get super-golden and crispy, tossing occasionally, then turn off the heat.\",\n",
    "  \"Meanwhile, beat the eggs in a bowl, then finely grate in the Parmesan and mix well.\",\n",
    "  \"Use tongs to transfer your pasta straight into the pan and toss with the bacon.\",\n",
    "  \"Pour the Parmesan eggs into the pan, and keep everything moving, loosening with splashes of the pasta cooking water until you have a silky sauce. Make sure the pan isn\\u2019t too hot otherwise the eggs will scramble.\",\n",
    "  \"Plate up the pasta, and finish with an extra grating of Parmesan.\",\n",
    "  \"Tips\",\n",
    "  \"EASY SWAPS\",\n",
    "  \"Use Cheddar cheese instead of Parmesan.\",\n",
    "  \"If you haven\\u2019t got any eggs, don\\u2019t worry \\u2013 the starchy pasta water will be enough.\",\n",
    "  \"Please enable targetting cookies to show this banner\"\n",
    "]\n",
    "\n",
    "get_substitutions(ingredients, doc2vec, doc2vec_vecs, instructions=instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ingredient_substitution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
